19-07-2020
_______________________________________________________________
S1E1

intro to course====>

used in survilence and others===>

face_recognition library==>OpenCv, Dlib are the library===>

will also cover basic python little bit====>

===>will test all installation===>

====>will detect emotions

====>deep learning to predict age and gender


_______________________________________________________________

S2E1

intro to face recognition technology using deep learning===>python


Baisc steps===>face detection==>find landmark like nose,lips etc

	=======>face allignment

	=======>feature Extraction

	======>face recognition (score more than threshhold)



_______________________________________________________________
S3E1

environment setup===>

python, numpy, Scikit learn, OpenCV(open source image processing),Dlib(deep learning), face-recognition,Cmake..>===>libraries

open source distribution====>Anaconda==>

===>google===>download Anaconda==>python 3.7 version===>466Mb for windows===>next==>next==>Add environment path variable (by checking at time of installing anaconda)===>IDE is spyder===>

>>>print("hello")===>press run on spyder==>

_______________________________________________________________
S4E1

python basics==>

===>String Assignment
===>Number Assignment
===>multiple assignment ==>a,b,c=10,11,12
====>None Assignment

_______________________________________________________________
S4E2

python data structure==>tuples==>Lists (append)===>Dictionary {"key":value} //(mydict.keys() // will return list)===>
print("%d" % num_var) // will also give result

_______________________________________________________________
S4E3

python flow control===>if===>elif condition: ===>for loop( // for i in range(10):)===>while condition: 

_______________________________________________________________
S4E4

python function===>

def fn_name():
	body_statements
	return resullt

_______________________________________________________________
S5E1

custom dependicies===>cMake===>Cross platform Make===>

===>launch anaconda prompt===>pip install cmake===>(took 5 mins)===>

openCv===>for real -time computer vision===>pip install opencv-python====>

===>next package===>Dlib is C++ toolkit===>

===>another library Face recognition===>by Adam Geitgey===>

====>we will be using wheel to install last above two packages

===================================================

set these as environment variables

 C:\Users\RajaRama\Anaconda3\Scripts

 C:\Users\RajaRama\Anaconda3

 C:\Users\RajaRama\Anaconda3\Library\bin

 if conda is not recognised in conda prompt

 ===================================================================
_______________________________________________________________
S5E2

setting up environment===>

goto pypi.org===>face-recognition===>download wheel file of this package===>

goto===>anaconda prompt===>change directory where library wheel file is present===>

cd path_of_wheel_file

===>pip install ##wheel_file.whl

cMake and dlib must be installed before installing face_recognition===>


import cv2
import dlib
import face_recognition

print(cv2.__version__) //# and similarly we can chack cersion of every package version==>
print(dlib.__version__)
print(face_recognition.__version__)
===>

_______________________________________________________________
S6E1

face detection===>detect and locate faces in the image===>

face detector===> HoG face detector and CNN based Detector included in DLib(convolution neural network)(more accurate)

HOG(histogram of oriented Gradient)===>detects pixel===>minimum size 80 * 80===>front side only==>

CNN face Detector===>slow on CPU==>fast on GPU==>detect multiple face orientation===>

_______________________________________________________________
S7E1

face detection implementation 1===>

cmake ===>to capture images and all===>

folder structure===>
face recognition===>images
		===>library
		====>code
			===>image_face_detection.py
			//==>import cv2
			//===> import face_recognition
image_to_detect = cv2.imread('images/filename.jpg')
cv2.imshow("test", image_to_detect) // to show and check image

all_face_location = face_recognition.face_locations(image_to_detect,model="hog | cnn") //can also pass upsample(number_of_times_to_upsample) by default its 1===>accuracy, return tuples
print("there are {} no of faces in this image".format(len(all_face_location)))

//### get face location===>

_______________________________________________________________
S7E2

//loop through faces

for index, current_face_location in enumerate(all_face_location):
	top_pos,right_pos,bottom_pos,left_pos = current_face_location //# goes in clockwise direction
	print('found face {} at top: {},right {},bottom {},left {}'.format(index+1,top_pos,right_pos,bottom_pos,left_pos))
	// can slice the image
	current_face_image = image_to_detect[top_pos:bottom_pos,left_pos:right_pos] 
	cvw.imshow("Face no "+str(index+1),current_face_image)


_______________________________________________________________
S8E1

// create a  new file
====>realtime_face_detection.py

webcacam_video_stream = cv2.videoCapture(0)  // by default 0 indicates original webcam attached(pre) to computer

// empty arrray of face location

all_face_location = [] // to hold all face location

while True: //keep on going unless we give some interrupt
	ret, current_frame = webcam_video_stream.read()
	current_frame_small = cv2.resize(current_frame,(0,0),(fx=0.25,fx=0.25)) //optional as we are scaling down the current fram by 1/4 (0,0)==>indicates we are not touching image size 
	// find total no of faces
	all_face_locations = face_recognition.face_locations(current_frame_small)
	for index,current_face_location in enumerate(all_face_locations):
		top_pos,right_pos,... = current_face_location // from previous code
		top_pos =top_pos*4 // to get actual frame
		right_pos=right_pos*4
		bottom_pos = bottom_pos*4 
		left_pos= left_pos*4
		cv2.rectnale(current_frame, (left_pos,top_pos),(right_pos, bottom_pos), (0,0,255),2) // last one is for thickness of rrectangle
_______________________________________________________________
S8E2

//cont..
in for loop onle which is in while loop itself
		cv2.rectnale(current_frame, (left_pos,top_pos),(right_pos, bottom_pos), (0,0,255),2) // last one is for thickness of rrectangle
	cv2.imshow("FAce no",current_frame) // if one title then one window //outside of for loop inside of while loop
if cv2.waitKey(1) & 0xFF = ord('q'): //in while loop not in for loop to exit			break
	break

//outside of while loop
webcam_video_stream.realease()
cv2.destroyAllWindows()

//note if img is far from camera then it may not detect face , hence we can upgrade "number_of_times_to_upscale=2"

//like // face_recognition.face_locations(current_frame_small,number_of_times_to_upscale=2,model="hog") //now check

_______________________________________________________________
S9E1

real time face detection and blurring===>

//current_face_image  = cv2.GaussianBlur(current_face_image,(99,99), 30) //(99,99)==>kernel size(blur size), standard deviation is 30

// paste image back to current_frame
current_frame[top_pos:bottom_pos,left_pos:right_pos] = current_face_image

_______________________________________________________________
S10E1

expression detecion===>install two neural network library===>TensorFLow and Keras (tensarflow.org and keras.io)

===>open anaconda prompt===>conda install tensorflow
===>conda install keras

===>on kaggle===>36,000 image comparison===>48*48 pixel===>kaggle.com===>have dataset===>download dataset===>

=====>inn code folder===>create dataset===>which will have json and ...model_weight.H5 file==>

_______________________________________________________________
S11E1

realtime_face_emotion_detection.py

import numpy as np
import cv2
from keras.preprocessing import image
from keras.models import  model_from_json

//switch on webcam
webcam_video_stream = cv2.VideoCapture(0)


face_exp_model = model_from_json(open("dataset/facial_expression_model.json","r").read())
face_exp_model.load_weights('dataset/facial_expression_model_weight.h5') // will give nos for emotions so lets map

emotions_label = ("angry","disgust","fear","happy","sad","surprise","neutral")

// slicing //current_face_image = current_frame[top_pos:bottom_pos,left_pos:right_pos]

//convert into grayscale==> current_face_image = cv2.cvtColor(current_face_image,cv2.COLOR_BGR2GRAY)

//resize to 48*48 px

current_face_image = cv2.resize(current_face_image, (48, 48))

// todo all calculation==>convert image into pixel (matrix)===>

img_pixels = image.img_to_array(current_face_image)

// convert it into 1-d array

img_pixels = np.expand_dims(img_pixels, axis=0)

=======>
//all this pixels are ranges from 0 to 255

so divide image pixels by 

=====> img_pixels=/ 255

_______________________________________________________________
S11E2

...cont
//do prediction using model, get prediction value ranges from 0 to 6 , it will have all the values we need to choose maximum occurence value
exp_predictions = face_exp_model.predict(img_pixels)

//find max occurence==>to test
max_index = np.argmax(exp_prediction[0])

// find label
emotion_label = emotions_label[max_index]

//display on image

font = cv2.FONT_HERSHEY_DUPLEX
cv2.putText(current_frame,emotion_label, (left_pos,bottom_pos),font,0.5,(255,255,255),1) //0.5 is scale, 1 is width of text

_______________________________________________________________
S12E1

facial expression detection of image===>

// get models



//shift+tab to reverse the effect of tab

_______________________________________________________________
S13E1

Realtime time age and gender detection==>

by Gil Levi and Tal Hassner using the Adience dataset

//Adience-data.html===>in talhassner.github.io

// caffe is a deep learning framework

files to download

===>Age Classification caffe Model
===>Age protext file
===>Gender classificatication caffe model
====>Gender classification protext file
====>The Mean Image

_______________________________________________________________
S14E1

real-time Age and gender Detection implementation

realtime_age_gender_detection.py===>remove blur code

while loop is for frame===>
for loop is for no of faces in frame faces===>


AGE_GENDER_MODEL_MEAN_VALUES =(78.4263377603, 87.7689143744, 114.895847746)

//create blob of image

current_face_image_blob = cv2.dnn.blobFromImage(current_face_image, 1, (227, 227), AGE_GENDER_MODEL_MEAN_VALUES, swapRB=False) //1 is scale factor, (227,277)==>size of blob, swapRedBlue=False

// gender_label_list = ['Male', 'Female']
//gender_protext = "dataset/gender_deploy.protext"
//gender_caffemodel = "dataset/gender_net.caffemodel"

gender_cov_net = cv2.dnn.readNet(gender_caffemodel, gender_protext)
gender_cov_net.setInput(current_face_image_blob)

//forward to examine using Artificial Neural Networks

gender_predictions = gender_cov_net.forward()
gender = gender_label_list[gender_predictions[0].argmax()]

/// now age prediction

age_label_list= ['(0-2)','(4-6)','(8-12)','(15-20)','(25-32)','(38-43)','(48-53)','(60-100)']

 // import all files related to age

age_protext = "dtaaset/age_deploy.protxt"
age_caffemodel = 'dataset/age_net.caffemodel'

age_cov_net = cv2.dnn.readNet(age_caffemodel, age_protext)

age_cov_net.setInput(current_face_imaeg_blob)

age_predictions = age_cov_net.forward()

age = age_label_list[age_predictions[0].argmax()]

cv2.rectangle(current_frame,(left_pos,top_pos),(right_pos,bottom_pos),(0,0,255),2)

font = cv2.FONT_HERSHEY_DUPLEX
cv2.putTest(current_frame,gender+" "+age+"yrs", (left_pos,bottom_pos), font, 0.5, (0,0,255),1)


_______________________________________________________________
S15E1

age and gender detection of static image

_______________________________________________________________
S16E1
68 face marks===for face detection

Face recognition===>128 face embodings or lanndmarks==>library uses openFace internally

_______________________________________________________________
S17E1

face recognition implementation==>

// import libraries
//load sample picture and get face encoding

modi_face = face_recognition.load_image_file('images/samples/modi.jpg')

modi_face_encodings = face_recognition.face_encodings(modi_image)[0]



trump_face = face_recognition.load_image_file('images/samples/trump.jpg')

trump_face_encodings = face_recognition.face_encodings(trump_image)[0] //[0] as we know that there is only one face

//create list to save encodings

known_face_encodings = [modi_face_encodings, trump_face_encodings]

//create array to hold labels

known_face_names = ["Narendra Modi", "Donald Trump"]

// load an image to identify faces

image_to_recognize = face_recognition.load_image_file("images/test/trump-modi.jpg") // it changes RGB

original_image = cv2.imread("images/determine.jpg")

//find all the faces and their encodings in unknown image which is to identify

all_face_locations = face_recognition.face_locations(image_to_recognize,model="hog")


all_face_encodings = face_recognition.face_encodings(image_to_recognize,all_face_locations) //

// loop through face location and encodings

for current_face_location, curent_face_encoding in zip(all_face_locations, all_face_encodings):
	top_pos,right_pos,bottom_pos,left_pos = current_face_location
_______________________________________________________________
S17E2

//compare faces and get the matcheslist(inside the loops)

all_matches = face_reconition.compare_faces(known_face_encodings, current_face_encoding) // will return index from known_face_encodings

name_of_person = 'unknown_faces'

//

	if True in all_matches:                                     //check if item is present at every step
		first_match_index = all_matches.index(True)
		name_of_person = known_faces[first_match_index]
	cv2.rectangle(image_to_recognize,(left_pos,top_pos),(right_pos,bottom_pos),(255,0,0),2)  // put  original_image in place of image_to_recognize as
	font = cv2.FONT_HERSHEY_DUPLEX
	cv2.putText(image_to_recognize, name_of_person, (left_pos,bottom_pos), font, 0.5, (255,255,255),1) // put  original_image in place of image_to_recognize as

_______________________________________________________________________________________________________________
S18E1

Realtime face Recognittion

realtime_face_recogniton.py

webcam_video_stream = cv2.VideoCaptue(0)

// get known face encoding
....

//real time checking

while True:

	//get the current frame from the video stream as an image
	ret,current_frame = webcam_video_stream.read()

	//resize the current frame to 1/4 to process faster
	current_frame_small = cv2.resize(currentt_frame,(0,0),fx=0.25,fy=0.25)

	#detect all faces in the image
	#arguements are image, no_of_times_to_upsample, model

__________________________________________________________________________________

S18E2
while True:

	.
	;
	.cont

	all_face_locations =  face_recognition.face_locations(current_frame_small,number_of_times_to_upsample=2)
	
	//get face encodings

	all_face_encodings = face_recognition.face_encodings(current_frame_small,all_face_locations)

	
	for current_face_locaion,current_face_encoding in zip(all_face_locations,all_face_encodings):
		top_pos,right_pos,bottom_pos,left_pos = current_face_location
		
		//resize to original frame by multiplying each pos by 4

		
		all_matches = face_recognition.compare_faces(known_face_encodings, current_face_encoding)
		name_of_person = "Unknown Face"

		if True in all_matches:
				first_match_index = all_matches.index(True)
			name_of_person = known_face_names[first_match_index]

		cv2.rectangle(current_frame,(left_pos,top_pos),(right_pos,bottom_pos),(255,0,0),2)
		cv2.putText(current_frame, name_of_person,(left_pos,bottom_pos),font, 0.5,(255,255,255),2)
	
		

	cv2.imshow("webcam",current_frame)
	if cv2.waitKey(1) & 0xFF == ord('q'):
		break

_______________________________________________________________________________________________________________________
S19E1

face distance===>earlier like it was true or false==>

//get encoding of images===>

_______________________________________________________________________________________________________________________
S19E2

...cont

face_distances = face_recognition.face_distance(known_face_encodings, image_to_recognize_encoding)

for i,face_distance in enumerate(face_distances):
	print("the calculated face distance is {:.2} from sample image {}".format(face_distance, known_face_names[i])) //{:.2} upto to decimal places

// trump to modi face_distance is 0.78
//trump to trump face distance is 0.46


can also claculate percentage matching===>by (1-float(face_distance))*100

_______________________________________________________________________________________________________________________
S20E1

face landmark visualisation===>

68 points to find face==>

jaw===>0 to 16
right Eyebrow==>17-21
left-eyebrow==>22-26
nose===>27-34
.
.
Mouth ==>48 - 67
========================

face encoding==>128 embeddings

===>pillow library===>python image library

close all open spyder and navigator ===>and type pip install pillow

_______________________________________________________________________________________________________________________
S20E2

from PIL import Image, ImageDraw

//load image which is munpy array
face_image = face_recognition.load_image_file("images/samples/abc.jpg")

//get land_mark_list
face_landmark_list = face_recognition.face_landmarks(face_image)

//draw landmark

for face_landmarks in face_landmarks_list:
	pil_image = Image.fromarray(face_image)
	d = ImageDraw.Draw(pil_image)
	d.line(face-Landmarks['chin'],fill=(255,255,255),width=2) // fill=(color_code)
	d.line(face-Landmarks['left_eyebrow'],fill=(255,255,255),width=2) // fill=(color_code)
	.
	.
	.
	do it for all land marks
pil.image.show()

// if want to save
pil_image.save("file path..")

_______________________________________________________________________________________________________________________
S21E1

we will be introducing 

RGBA===>alpha parameter with RGB==>i.e opaqueness

d = ImageDraw.Draw(pil_image,"RGBA")

d.polygon(face_landmarks['top_lip'],fill=(150,0,0,128))

###################################################################








