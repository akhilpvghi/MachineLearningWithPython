		ADW notes

https://www.analytics8.com/insights/data-warehouse-creation-basics/

Dt 21-08-2020


Waeka tool and penta house software===>

Date warehouse ===>process of creating and using data 
		warehouse collecting it from heterogenous 
		source(external sources)

		===>very large collection of data base (can be in tera bytes(TB))

		===>data that we get must be normalised

		===>can not be imagine/Analysed in tabular form (can be any type of data)

		===>manage by Integration tool (Organised way)

		
Data Base==>set of information which can manage inforamtion

ETL===Extraction Transform Loading

ETL is short for extract, transform, load,
three database functions that are combined 
into one tool to pull data out of one database 
and place it into another database. Extract is the 
process of reading data from a database


OLTP ====>online transactional process ===>Online

_____________________________________________________________________________________________
24-08-2020

precision in data base==>current data==>

1st Normal form===>talks about atomicity
2nd Normal form===>full functional dependency
3rd normal form===>talks about transitivity

Problems==>
fully normalised data==>inefficiently

Integration system technique ===>must have converion process

_____________________________________________________________________________________________
25-08-2020

problems with data warehouse 

===>Data Quality Issues

1)Different levels of Granularity ===>summarise
2)drill-down capability


Data Warehouse??

single,complete,consitent obtained from a variety of sources
made avlbl to user in such a way that it can be used in business

==>subject-oriented (S)===>insurance e.g. ,car purchasing e.g. etc
-->integrated (I)====>must have a single
-->time varying (T) 
--->non-volatile (N)===> can't be manipulated

(shortform ===>NSIT)

_____________________________________________________________________________________________
26-08-2020

DW===>

3-Tier Architecture

1)
Information Sources===>data cleaning,extract

2) Data warehouse Server===>put data into DWS ===>can have more than one data marks in DWS
(Tier-1)

3)OLAP (Online analytical processing) Servers ===> consist MOLAP(multidimensional OLAP)
			and ROLAP(Relation OLAP)
			
4)CLients ===>analysis at this end


_____________________________________________________________________________________________
28-08-2020


Strategic information (for e.g. college info before admission) ==> information system used in bussiness initiative, decision creation

====>deep knowledge

_____________________________________________________________________________________________
31-08-2020



flight system==>strategic information

4 component of DSS(Decision Support System)

==>DAta Store
==> Data Extraction and filtering
==> End user query tool




_____________________________________________________________________________________________
07-09-2020

Architectural 5 types


central data warehouse===>
OLTP===>online transactional process

DImensional Analysis==>usage of information is unpredictible

		==>user can tellyou how they think about the business



_____________________________________________________________________________________________
14-09-2020

Loading Dimension	===> whenver we talk about dimensional it came from OLTP===>we talk about modification
			
			===>we consider it constant so we dont change prodeuct key, or primary key , we can change description

			===>type 1===>we change attribute data===>easiest one




type===>0,1,2,3,4,5


_____________________________________________________________________________________________
22-09-2020

ETL==>

_____________________________________________________________________________________________
29-09-2020

Join Vs merging



_________________________________________________________________________________

https://www.youtube.com/watch?v=KL331JCvNHI&list=PLV8vIYTIdSnb4H0JvSTt3PyCNFGGlO78u&index=34

L-34

Dimensional modelling==>multidimensions ===>measures and dimensions

	measures ===>measure some value and can be aggregated


Facts ==>numeric value /measure ===>can analyse relationship b/w dimesions

Dimesion ==> logical related attribute ==>


_________________________________________________________________________________

L-43,44,27,29,28,51,52 ---link analysis,53,54,26

Data mining


_________________________________________________________________________________________________________________________________

L-43

Introduction to Data mining ===>discovering or mining knowledge from large amount of data.

	===> Knowledge discovery data (also known as KDD)

	===>attempts to extract hidden pattern

	===>supports automatic exploration of data

Need--
	==> Extracting meaningful data
	==>finding hidden information
	==>to manually analyse large amount of data
	===>we need automatic analysis of data

Evolution of data mining
	==>statistics ---> consist regression analysis, cluster analysis, standard deviation etc. [lay the foundation of data mining]

	==>Artificial Inteligence (AI): Applying of human thoughts

	==>MAchine learning = union of AI and statistics
			===>learn from data



_________________________________________________________________________________________________________________________________

L-52 -->Link analysis in Data mining

like airlines(like cities) etc.

===>useful for finding patterns in relationship

e.g bread with better...

Application
	==>Association Discovery
		==>support  and confidence

	==> Sequential Pattern Discovery(we also consider time in this discovery)
	
	==> SImilar time Sequence Discovery
		==>deriving sequence using another sequence



_________________________________________________________________________________________________________________________________

L-51

Neural Networks ==>

	===>mimics the human brain by learning from training data set
		and applying the learning to generalise patterns for classification 
		and prediction.

i/p	weight		summarization unit

x1  ->	w1  ->		 process            

x2  ->  w2  ->

					

sum = X*W
     =summision of(xiwi)

Layers
	==>input layer ==>contains input unit
	==>Hidden layer ==>provide non linearity to the network
	==>Output layer 



_________________________________________________________________________________________________________________________________

L-53

Genetic algorithm ===>similar to neural network
		====> every model competes with every inheriting traits from previous one
			until only the most predictive model survives

			e.g how many mail should be sent to grow business
			


_________________________________________________________________________________________________________________________________

L-12

K-means algorithm==>

==>e.g do it as ywo cluster

calculate eucledian distance

1) Initialise two clusters

	==>calculate centroid


_________________________________________________________________________________________________________________________________

L-27

Memory based resoning (data mining technique) ===>
	
		==>instances of a model to predict unkown instances
			==>maintains a dataset of known records
			==>ALgo knows the characteristics of the records in this training set
			==>when new record arrives for evaluation, the algo finds the neighbors to the new record, then uses characteristics of the neighbors for prediction and classification.

	-->distance between new records and records in the training data set is calculated

ISSUES
	===>finding suitable records
	===>establishing way to compose records
	===>distance function and combination functions


_________________________________________________________________________________________________________________________________

L-28

Decision Trees (Data mining major techniques)

	==>applies to classification and predicrtion
	==>used to visually define rules which are simple interpreted and understood


	Root Node (Main Question)
	   |
	Branch Nodes (Intermediate nodes)
	   |
	Leaf Nodes (Answer)

factors
	==>Information gain ===>measure of how much the answwer to a specific question provides
	==>Entropy ===>measure of uncertainity there is in information

	###information gain is inversely proportional to entropy

e.g ID3 Algorithm
e.g credit score example

ISSUE
	==>finding splitting attribute

_________________________________________________________________________________________________________________________________

L-29

Clustering Detection(major data mining technique)
	===>unsupervised learning (we don't know the class labels and no. of labels)
	===>earliest data mining technique
	===>searches for similar data elements
	===>e.g. K-means cluster

	Distance measure
		==>Euclidean distance

Advantages of using clustering
	===>not affected by new object
	===>no need to worry about sign
	===>computation process is simple

Application of clustering
	===>Establish taxonomy of disease,cure and symptoms
	===>Social network communities
	===>Seismology ==>Epicentre of earthquake
	===>Business marketting==>targetted customer finding

_________________________________________________________________________________________________________________________________

L-12

K-Means clustering
	==>using eucledian distance divide into clusters

	===>observed value 
	===>centroid value

	==>after adding new point to cluster the centroid will change again

	==>centroid = (a+b)/2


_________________________________________________________________________________________________________________________________
https://www.youtube.com/watch?v=OTt8z8VAQCw

Aprior algorithm ==>algorithm for mining frequent itemset n given transaction

	===>uses bottom-up approach

	==>rejection takes place based on minimum support

	
	L1, L2 ...are formed after rejection

	===>now form association rules

_________________________________________________________________________________________________________________________________

L-13

K-Means clustering

c1={Row #, }
c2={Row #, }
	
c1 and c2 are divided clusters

--->to be done..

_________________________________________________________________________________________________________________________________

L-14

P(A|B) = (P(B|A)*P(A))/P(B)


_________________________________________________________________________________________________________________________________

L-15

P(A|B) = (P(B|A)*P(A))/P(B)
_________________________________________________________________________________________________________________________________

L-37 (UNIT-2)

OLAP -->introduced by E.F Codd

	===>analysing historic and time variant data

	===>complex data derived

	==>business datwarehouse

	==>can be alalysed by data mining, Analytics Decision making

	==>Information


FEATURES OF OLAP:-
	1) FAst

	2) Analysis: lets user to enter query in interactive tool
	
	3) Shared

	4) Multidimensional ===>measure and dimensional

	5) Information

OLTP-->characterised by large no. of short online transaction

	==>business process

	==>operations


_________________________________________________________________________________________________________________________________

L-38 (UNIT-2)

OLAP OPerations

	1) Pivoting ==>changing from one dimesion orientaion to another or ROTATION

	2) SLICE and DICE ==>   slice =>cross tabulation is done for specific value other than all for fixed third dimension
				dice ==>in dice two or more dimension are fixed

	3) ROll up and Drill down ==>roll up ==>individual to confined data==>finer granularity to confined data
				     drill down ==>confined data to finer granulaity (like breaking quarter into months)

_________________________________________________________________________________________________________________________________

L-39 (UNIT-2)

Diff b/w OLTP and OLAP

OLTP

	==>very fast

	==>aim is to control & run basic business task

	==>highly normalised database

	==>simple and standard queries
OLAP

	==>speed depends on data

	==>decision support

	==>data in denormalised form

	==>deals with consolidated data(hetrogenous data source)

	complex queries with aggregation data


OLAP

	===>multidimensional

	===>transparency ==>to improve efficiency and production

	==>Accessibility ===>provides restricted access

	===>consisitency ===>handle managemnet when increases(should not effect other aspects)

	===>client server model

	==>Generic Dimensionality

	===>Dynamic sparse matrix (distribution of data dynamically)

	===>Multiuser 

	===>Unrestricted

	===>Intuitive manipulation

	===>FLexible



_________________________________________________________________________________________________________________________________

L-45 (unit-3)

DAta Transformation (data cleaning aspect)
		==>it is data preprocessing technique that transforms or consolidate the data into 
		alternate forms appropraiate for mining


INVOLVED PROCESSES (# SANG)
	1) SMoothing: Removing the noise from data (binning, regression, clustering)

	2) Aggregation: Summary or aggregate function
			==>constructing a data cube (helps in OLAP)

	3) Generalization: low-level concepts are replaced with higher level

	4) Normalisation : Attributes value are normalised by scaling their values so that they fall in specified range.

			==> Normalisation methods
					
					1) Min-Max Normalisation: V' = (V-min(x))/max(x)-min(x)

					2) Z-Score Normalisation  ==>(zero mean normalisation) V' = V - (mean of attribute)/ standard deviation of attribute


_________________________________________________________________________________________________________________________________

L-49 (unit-3)

Dimensionality REDUCZTION

	==>it represents the original data in the compressed or redusec form by applying data encoding or transformation

		LOSSLESS ==>if original data can be reconstructed from compressed data without losing any information

		LOSSY	===> if reconstructed data is the approximation of compresses data

Numerosity Reduction 

	==>It reduces the data volume choosing smaller form of representation

		PARAMETRIC ==>only parameters of data and outliers are stored instead of actual data...e.g Regression, log-linear models

		NON-PARAMETRIC ===>data is stored in form of histograms, clustering, sampling etc


_________________________________________________________________________________________________________________________________

L-50 (unit-3)

Data Discertization: (concept hierarchy)

	==>divides range of attributes into intervals so as to reduce numer of values for a given continous attribute.

	TECHNIQUES
		
		==>Splitting : top-down(attribute is splitted into range of values)

		==>Merging : Bottom-up (initially we consider later remove some during merging)

		===>Supervised ==>class information is known

		===>Unsupervised ==>class information is not known


CONCEPT HIERARCHY

	===>it also helps in reducing data

	==>by replacing low-level concepts with higher level concepts.

	mobile no. and landline no. ==>low level

	telephone ===>higher level

how they are applicable on numerical data

	1) binning ===> sepearate in different bins

	2) Histogram

	3) CLuster analysis

_________________________________________________________________________________________________________________________________

L-16

Y= aX+b

a= n*sumsn(xy)-sumsn(x)sumsn(y)/n*sumsn(x^2)+ (sumsn(x))^2

b = 1/n(sumsn(yi)-a*sumsn(xi))




_________________________________________________________________________________________________________________________________

L-17

Y= aX+b

a= n*sumsn(xy)-sumsn(x)sumsn(y)/n*sumsn(x^2)+ (sumsn(x))^2

b = 1/n(sumsn(yi)-a*sumsn(xi))

_________________________________________________________________________________________________________________________________

L-20

Data mart ==>data store which is designed for a department of an organistaion, 
or data mart is subset of the D.W that is usually oriented to a specific purpose.

#diagram

Reason for Creating Data mart
	
	===>Easy Access of frequent data
	===>Improved end-user response time
	===>Easy creation of data mart
	===>Less cost

Different type of Data Mart

	DEPENDENT DATA MART: In this data mart is built by drawing data from central data datawarehouse that
				already exists.

	
	INDEPENDENT DATA MART: In this, the data mart is built by drawing from operational or
				external sources of data or both.

				==>Data mart is created without help of Data warehouse


ADVANTAGES OF DATA MART:

	1) Simpler, more focussed & flexible
	2) Low cost s/w and H/w
	3) Faster and cheaper to build
	4) stores data closer that enhances performance



Disadvantages of Data mart

1) unorganised development
2) difficult process of data access, cleansing
3) NOt much thorough design as data warehouse
4) increase in data mart size leads to problem such as performance
	degradation, data inconsistency.

Difference  between data warehouse and data mart


data warehouse
	==>corporate wise
	==>union of Data mart
	==>recives functions from staging area
	==>centralised control and management of data
	==>Keeps all historical data
	
data mart
	===>departmental wise
	==>subset of DW
	==>it has star join concept means fact and dimension
	==>Technology optimal for dat access and analysis
	==>STRUCTURED to suit the departmental view of data

_________________________________________________________________________________________________________________________________

L-21

STEPS IN IMPLEMENTING DATA MART (#MAP CD inverse way)

	Designing
		==>first step and covers all the task from initiating the request for data mart through gathering
		information about the requirement
			==>gathering business/technlogy information
			==>identifying data sources
			==>selecting appropriate subset of data
			==>Designing both logical/physical str.
	2) COnstructing		
		==>include creating physical database and logical structure associated with the data mart.
			==>creating db, tablespaces, other reltn
			==>creating schema object =>table,index.
			==>Determining best table access strategy.
	3) Populating (ETL)
		==>mapping of data sources 
		==>extracting the data
		==>cleansing and transforming the data
		==>loading the data

	4)Accesing 
		==>it involves putting the data to use
		==>frontend development
		==>maintainging and managing bussiness interfaces
		==>setting up db structure==>summarised table.

	5) Managing
		==>involves managing the data over its lifetime
			==>Secure access the data.
			==>Managing the growth of data
			==>optimising the system for better performance
			==>data availability

_________________________________________________________________________________________________________________________________

L-22

DATA WAREHOUSE

	NEED
		==>for taking quick and effective decision
		==>provides necessary information
		==>creating and managing efficient data repository


	GOAL OF Data warehouse
		===>secured and easy access of infon to user.
		==>consistent informationto be improved
		==>clean and authentic data for analysis
		==>collected data must be accurate, verified.
		==>data must be adaptive in nature
	
	ADVANTAGES
		==>historical informtn is provided to analyst.
		==>Increase quality of data
		==>helps in recovering from DB failure
		==>helps in data mining process


	BENEFITS
		==>increases in product inventory turnover
		==>decrease in cost of productn
		==increase sales with improved target market selection
		==>better business intelligence

Difficulties/problem

	1) construction of D.W for large organisation is a complex task 
		and can take years to complete(is accomodate to changes.)

	2) Administration of D.W is also complex and requires higher level skills and team with technical expertise.(complexity increase with size)


	3)Quality conrol of DW issues both quality and consistency of data.(data should be correct,clean and authentic)


_________________________________________________________________________________________________________________________________

L-24

Distributed DW

	==>Data are shared across multiple data repositries,
		for the purpose of OLAP and where each data warehouse
		may belong to one or more organisation.

	==>It covers a complete enterprise data warehouse but have tiny data that
		that are built seperately.

	==>These data stores are connected physically over a network to provide users access 
		to the relevant reports.

	==>It is the core of all enterprise data which is used to send relevant data to 
		individual data marts.

Categories of distributed data warehouse
	
	1)Local and global data	warehouse: 
		data is unique to local Operating system

		global is integrated data

	2) Technologically distributed DW:
		logically single data warehouse but physically 
		many distributed data warehouse 

	3) Independently evolving distributed DW:
		uncordinated environment DW ==(DW increases one by one)
		
ADVANTAGES AND DISADVANTAGES OF Distributed DW

	1) Faster : local DW has its own control

	2) No Limits: 

	3) Cost is much less than centralised structure

Disadvantages of distributed DW

	1) Metadata : overhead increases (info of metadata) as data increases


	2)multiple development efforts


	3) The roles and responsibilities are not clearly defined
	4)Excessive network traffic starts
	5) coordinating development becomes complex and less effective

	6) Interconnectivity could be problematic in case of traffic congetion






_________________________________________________________________________________________________________________________________

L-33

Extract, Transform, Load (ETL)

Extracting
	==>it is process to collect data from various source then transform and then load
	==>1st is extracting the data==>brought up from external sources
	==>Involves connecting to source, collecting necessary data needed for analytical processing.

	problems:  Data integrity(different names for same thing)

Transforming

	==>fit the data as std.
	==>series of steps are performed on extracted data to convert into standard format.
	==>conversion, cleaning duplicates, filtering the data, standardizing the data



Loading
	==>send the data to warehouse
	==>imports transformed data into large database or date warehouse


# diagram

data sources ==>ETL ==>Data warehouse staging ==>Data warehouse ====> then into different Data mart



Comparison of OLTP and data warehouse

data warehouse

	==>Adhoc queries(we dont know intially but on run time)
	==>regular updation using ETL tools (may be monthly)
	==>generally uses denormalised data
	==>Query accesses thousand or millions of records
	==>store historical data

OLTP 
	==>supports only predefined operations
	==>always upto date
	==>fully normalised data
	==>few records at a time
	==>No historical data	


_________________________________________________________________________________________________________________________________

L-40

Difference between fact table and dimesion table


Fact table 
		==>contains numerical values
		==>primary key of fact table is formed from primary key of dimension table
		==>provides measurement of an enterprise
		==>large size
		==>pure fact table is collection of forigner key

Dimension table

		==>character value(like book name)
		===>Each dimension table has its own primary key
		==>context/descriptive information
		==>smaller than fact table
		Pure dimension table is collecytion of primary key

=============================================================================

Multidimensional data modelling
		===>data stored in multidimesional database
		===>storage is like cube
		===>
		===>Data is in denormalised form
		==>Mainly used for analytical purpose(OLAP)
		==>MDX lang uage is used to manipulate data
		==>Non-volatile and time Invariant

Relation data modelling
		==>data is stored in RDBMS
		===>2-d Tables
		==>Normailsed
		==>OLTP
		==>SQL
		==>late data
		==>volatile and time variant

_________________________________________________________________________________________________________________________________

L-41

KDD V/S Data mining

KDD is the overall process that involves various steps such as 

		1) Selection
		2) Data Cleaning
		3)Preprocessing
		4) Data Transformation
		5) Data mining and so on ===>is one of the step in KDD process whereas KDD is overall process

	====>Extraction of useful and understanding knowledge

===>in data mining ==>we identifying patterns from data


======================================================

DBMS V/S Data mining

DBMS
	===>
	==>Query language
	==>can work alone without Data mining
	==>Basic elements==>which language, data structure, transfer mechanism


Data mining
	===>extracting interesting and unknown information from raw data. 
	===>automatic searching of data
	===>may not work with DBMS	
	==>basic tasks ==>classification, Regression, Clustering, Association

==========================================================

OLAP v/s Data mining

OLAP
	==>data summarisation takes place
	===>prior knowledge about information is required
	===>Summarised Data
	==>gives answer to question on past information
	==>small no of attributes
	==>less size of data
	==>user driven
	==>widely used

Data mining 
	===>automated discovery of pattern and knowledge
	==>No prior knowledge
	==>Detailed data
	==>can predict future
	===>many dimension attributes
	===>very large data
	==>data driven
	==>emerging

_________________________________________________________________________________________________________________________________

L-42

Dataware house v/s data mining

data warehouse
	===>integrate data from vaious sources
	==>organisation with storage medium.
	==>must take place before Data mining
	==>carried out by engineers
	e.g social website ==>gathering all information about user and store them in central repository

Data mining 
	==>extaract and identifying useful pattern and relationship from huge data
	==>techniques are applied on data to discover useful pattern
	==>After data warehouse process
	==>bussiness user with help of engineers
	==>e.g extracting information from data==>like user interest


_________________________________________________________________________________________________________________________________


L-32

OLAP Server model (MOLAP) ==>multidimensional OLAP

	==>has 3-tier architecture
	==>uses specialised data structure multidimensional dtabase
	==>Arrays are used (array value = cell loctn)
	==>compression is used
	==>e.g oracle Express server

# diagram

flat files and OLTP==>	MDDB(multi dimensional DB) LOader ====>multi-dimension DB cube ==>OLAP server==>user by WEB/LAN

Advantages
	===>Query response time is very fast==>as it has pre -aggregated data
	===>complex calculation (easy)
	===>less disk space(compression)
	===>Retrieval of data==>FAST[slice n DICE]

Disadvantage
	==>handle only limited of data
	==>large and additional investment
	==>high complexity
	==>Serious overhead oin processing or loading of data
		


_________________________________________________________________________________________________________________________________

L-30

OLAP server model ROLAP

	ROLAP(relational OLAP)==>ROLAP has  a 3-tier architechture
	==>works directly with relational database
	==>FACTS and DIMENSION Table are stored as Relations.
	==>New Relations are storing aggregate information


flat files ===> ETL server ===>DW star schema ===>OLAP server==>end user

#ROLAP structure

Advantages
	==>handle large amount of data base
	==>2-d relational table can be viewed in multi-dimensional form
	==>database security through form authorisation
	==>Any SQL reporting tool can access data

Disadvantages:-

	===>difficult to perform complex calculation.
	===>Long QUery time for large data size
	===>Additonal development time & more code support are needed
	===>doesn't have complex and complicated function


_________________________________________________________________________________________________________________________________

L-31

OLAP server models (HOLAP hybrid OLAP): this system includes the best of ROLAP and MOLAP

flat files ===>ETL server ===>DW schema ==>OLAP Server

Comparison OF OLAP Servers


			ROLAP			MOLAP			HOLAP


Detailed data		Relational DB		MDDB			Relational DB
storage locn


Aggregate data		Reln DB			MDDB			MDDB
storage Locn


Space requirment	Large			medium			Small


Query response time	slow			fast			medium	


Processing time		slow			fast			medium

latency			low			high			medium


_________________________________________________________________________________________________________________________________

L-23

Data warehouse architecture

2-Tier Architecture
	==>client server architecture
	==>sepearate physical data source and DW
	===>not expandable
	==>doesn't support large no. of end user
	==>Easy to maintain
	==>fast Commn.


#diagram
   
	req/res
cleints=======>>data source 
		(DW)

============================
3-Tier Architecture

Bottom tier
		===>has data base in 3-Tier architecture
		===>after cleaning,transformation data is loaded

Middle Tier
		==>OLAP server
		===>business logic is there

Top Tier
		===>front-end client layer.
		==>Query tools

#diagram
end users ===>OLAP========>Data source
(top)		(middle)	(bottom)

==================================

4 Tier architecture

	===>database
	===>Application
	===>Presentation
	===>Client

#diagram

end user ===> controllers  ===>view ===> business objects and models ===>data objects <==> Database

		(    Presentation   )       (business logic )


_________________________________________________________________________________________________________________________________

L-26

Data mining application

	==>customer segmentation ==>to understand customer

	==>market basket analysis	==>used in retail industry (buying pattern)

	==>Risk Management	==>used in insurance companies
				==>credit score	

	==>Fraud Detection	==>used by credit cards company
				==>like abnormal spending

	==>Demand prediction
				==>Reatil and online 
				==>like inn stock management

BENEFITS

	==>manufacturing	==>uncovering of variations b/w purchase order

	==>mail order
		==>mail promotion

	==>supermarket
		==>sell together

	==>Airlines
		==>discount

	==>Department store
		==>like during festivals


	===>Insurance
		===>save money by detecting frauds

	===>Banks
		==>increase business by direct marketting


DATA MINING TASKS

	PREDICTIVE TASKS
		==>predict values of data by making use of known results from a different set of sample data
	
		CLASSIFICATION
			===>predefined set of classes
		REGRESSION
			==>Forecasting future data values
		PREDICTION
			==>predicting future state (like flood)
		TIME SERIES ANALYSIS
			==>predicting future values based on time(stock market)
		
		
	DESCRIPTIVE TASKS
		==>enables you to determine patterns and relationship 
			in a sample data
		CLUSTERING
			==>No classes known (Grouping)
			(Marketing)
		SEQUENCE DISCOVERING
			==>similar to ARM but time dependent(web page linking)
		SUMMARIZATION
			==>subset of data (TRS value-TV viewership)
		ASSOCIATION RULE
			==>market baket analysis

		Mining
			==>Analysis


_________________________________________________________________________________________________________________________________

L-34

Dimensional modelling

	==>data in a DW is multidimensional having measure and dimension attributes.
	===>measure attributes ==>meaure some values and can be aggregated upon those values.
	==>DImension attribues==>those attributes which defineson which the measure attributes and their summary 

FACTS
	==>facts re the numerical measure by which one can analyse relationship b/w dimensions
		the relation containg such multidimensional data are called FACT TABLE


DIMENSIONS
	==>collection of logically related attributes and is viewed as an axis for modelling the data.
		A DIMENSION TAble is a table associated with each dimension helps in describing the dimension further.

_________________________________________________________________________________________________________________________________

L-35

DATA WAREHOUSE SCHEMA
	==>schema is a collection of database object, including tables, views, indexes etc

1) STAR SCHEMA OR STAR JOIN SCHEMA :- It consist of FACT Table with a single table for each dimension(dimension table)

Advantages
	==>Simplest and easiest 
	==>it optimizes the navigation through DB
	==>SImple queries
	==>Suitable for query processing

SNOWFLAKE SCHEMA
	==>cariation of star schema , which has multi level of simension table.[Dimesion tables are normalised]

Advantages

	==>less redundance due to normalised dimension table
	==>Dimesion table are easier to maintain

Disadvantages==>complex schema


FACT CONSTELLATION SCHEMA
	==>also known as galaxy schema. In this multiple FACT Tables shares the DIMENSION TABles

disadv
==>complex as it is having multiple fact tables
==>difficult to manage
==>Dimesiona tables are very large

STAR v/s SNOWFlake schema

Star 
	==>highly denormalised
	==>category wise single dimension table
	==>more data dependency and redundancy
	no need of complicated join

SNowflake

	==>normalised (in dimesion table)
	==>dimension table further splits into additional table
	==>less data dependency and redundancy
	===>complicated joins are required

_________________________________________________________________________________________________________________________________

L-36

Type of keys in STAR SCHEMA

PRIMARY KEYS: Each row in a domension table is identified by a unique
	value of attributes designated as primary key of the dimension.

SURROGATE KEYS: The surrogate keys are simply  system generated sequence numbers.
		===>no built-in meanings

FOREIGN KEYS: Primary keyof each dimension table must be a froreign key in 
	the fact table.


_________________________________________________________________________________________________________________________________

L-46

Data preprocessing 
	==>to improve quality in Data warehouse
	==>increses efficiency
	==>ease of mining process
	==>Removes Noisy data, inconsistent data and incomplete data
		==>data with missing values.

Data cleaning
	==>It cleans the data by filling in the missing values, smoothing noisy data
		,resolving the inconsistencyand removing the outliers.

WAYS TO HANDLE MISSING DATA DURING CLEANING
	==>manual entry of missing data
	==>using the attribute mean
	==>using most probable value ==>using Decion tree, Regression (in this we are predicting)
	==>using global constant(like N/A or unknown)
	==>ignore the tuple


_________________________________________________________________________________________________________________________________

L-47

Data Integration (Data preprocessing method)
	==>it is preprocessing method that involves merging of data from different sources (flat files, multi dimensional database, data cubes) in order to 
	form a data store like data warehouse.

ISSUES IN DATA INTEGRATION
	==>schema integration and object matching
		==>like attribute name mismatch
	==>Redundancy
		==>unwanted attributes
	==>Detection and resolution of data value conflicts
		==>$ and rupees converter need to modify


_________________________________________________________________________________________________________________________________

L-48

Data Reduction
	==>preprocessing technique that helps in obtaing reduced representation of dataset from the 
	available data set.
	==>Integrity of the original data should even after redn in data volume
	==>it should produce same analytics result as on original data



	Data cube aggregation
		==>process in which information is gathered and expressed in a summary
		form.
		==>dataset in smaller volumes
		==>used for statistical analysis.

	Dimensionality reductn 
		==>we remove redundant attributes.
		==>data compression
			==>encoding of data
		==>Numerosity reduction
			==>data is replaced by estimated value/alternative
		==>Discertisation and concept hierarchy
			==>data is replaced by range of values or higher conceptual level

_________________________________________________________________________________________________________________________________


L-25

Decision tree numerical


	info gain = I(P,n) = -(sumsn(i=1 to n)(P/S) *log2(P/S) + (n/S)*log2(n/S))


	S= P+n

	E = sumsn(i=1 to v) (pi+ni)/(p+n) I(p,n)
	
	Gain(A) = I(P,n)-E(A) 



find splitting attribute

	==>select attribute with highest gain 


_________________________________________________________________________________________________________________________________


L-19

_________________________________________________________________________________________________________________________________

L-11

FP growth algo

Item	freq	 priority

remove item which have freq less than given support

==>assign priority as higher freq= higher priority


============

trans id		ordered item considering priority


==>now draw fp growth tree


_________________________________________________________________________________________________________________________________

L-43

Data mining introduction

	==>it is process of discovering or mining knowledge from a large amount of data

	==>extracting useful information for business purposes

	==>extract hidden patterns and trends from large database


Evolution of data mining

	==>data mining term introduced in 1990

	==>Statistics ==>Regression analysis, cluster analysis, Standard deviation etc.
		[lay the foundation of data mining]
	
	==> Artificial intelligence ==>applying of human thought like processing


	==>Machine Learning => union of statistics and AI
			===>Learning by the software about data


_________________________________________________________________________________________________________________________________

L-44

Steps in Data mining/phases of KDD in database

	1) Data selection
		==>retrieves the data from various sources which is relevant

	2) Data Preprocessing
		==>consistent state
		==>removal of unnecessary information

	3) Data transformation 
		==>converting data into standard/suitable format

	4) Data mining
		==>by applying various algorithm

	5) Pattern evaluation
		==>pattern to knowledge

	6) Knowledge presentation
		==>data visualisation

	
Architecture of data mining system
#diagr

	various data sources
		|	=====>data cleaning,integrate, selection
	database or DW server
		|
	Data mining engine
		|
	Pattern evaluation
		|
	User interface
	


_________________________________________________________________________________________________________________________________

L-54

Asoociation Rule mining (ARM)

	==>used in retailing
	==>also known as market basket analysis

	A (antecedent) => B (consequent)

	Support and confidence

	Support ==>percentange of transaction
		P(A intersection B) ==>measures the frequency of association

	Confidence ==> C = P(B/A) = P(A intersection B)/ P(A)

		===>measures strength of association


Parameters
	1) FInding all parameters that appears frequennctly in transaction ==
		==>min. support count

	2) finding strong association among frequent items
		==>confidence

Problems in ARM
	==>finding strong asscoiation
	
FUnctions 
	==>collating information from numerous transn.
	===>genrating rules from counts in transaction


APriori Algorithm
	==>genrate candidate itemset of a given size 
	==>less than support value is eliminated
	==>iterative process
	


_________________________________________________________________________________________________________________________________


L-03

Data warehouse features		(# NSIT)

	defn of Bill Inmon
		==>subject oriented
		==>Integrated
		==>Non-volatile
		==>time variant


subject oriented
	==>saved as subjects
	==>like total sales

	

Integrated
	==>data in DW comes from several operational systems.
	==>like itegrating different types of account

	==>Remove inconsistency
		==>naming convention
		==>different codes
		==>data attributes
		==>measures

	==>Transformation
	==>Integration of source data


Time variant Data
	==>has to contain historical data, not just current value.
	==>contains time element
	==>allows the analysis of past
	==>relates info. to the present
	==>enables forecast to future



Non-volatile
	==>not update/delete from DW in real time

Data Granularity
	==>in DW, it is efficient to keep data summarised at different levels
	==>like monthly, quarterly etc


_________________________________________________________________________________________________________________________________


L-05

KNN classification numerical

eucledian distaance is calculated using observed value and actual value

==>calculate k distance, and check class according to k distance

_________________________________________________________________________________________________________________________________


L-06

KNN numerical 

if value of k is not given, then based on only one distance, we can classify the class

_________________________________________________________________________________________________________________________________

L-07

FP growth algorithm numerical

item	freq	priority

# higher the freq, higher the priority(or we can also say it as rank)

==>order the item as priority

==>root is always null in FP-tree

_________________________________________________________________________________________________________________________________

L-04

Data warehouse components

#diagram

Source		Management & control		Information Delivery
Data		

		DW				Data mining
		DBMS				OLAP
		Data marts			Report Query
Production	multidimensional
data

Internal
data

Archived 
Data

External
Data

		Data
		Staging

		  ETL


_________________________________________________________________________________________________________________________________

L-01

Aprior algorithm

support = freq/total transaction

confidence (A->B) = supp(A unioun B)/support(A)

_________________________________________________________________________________________________________________________________


L-02

Aprior algorithm

with three itemset rule

_________________________________________________________________________________________________________________________________


L-25

Noise in data

random error or variance in a measured variable

Techniques to remove noise

1) Binning :

	1) sort the data 

	2) divide the data into bins(bucket of range of values)

	3) bin size should be same

		1) smoothing by bin means

		2) bin medians

		3) bin boundaries

2) Regression
	:Prediction(used to fit an equation to a data set)

	Linear regression = y(predicted value) = mx(given value) +b


3) Clustering:	

steps of data cleaning process
	1) detection of discrepancy
		==>manual error
	2) transformation of data



_________________________________________________________________________________________________________________________________

L-08

Agglomerative algorithm

plot a dendogram

==>using single linkage

===>dendogram is like bar graph


_________________________________________________________________________________________________________________________________

L-09

Agglomerative algorithm

plot a dendogram

==>using single linkage

===>dendogram is like bar graph

in each step find closest pair cluster and merge them

==>can draw cluster based on that dendogram

_________________________________________________________________________________________________________________________________

L-10

agglomeraive clustering

in case of complete linkage, we select maximum of distance





















