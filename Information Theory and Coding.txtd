
Information theory and coding

L1
______________________________________________________________

intro to information--->categorise if important or not--->

or prioritize information

Shanon paper-->the matheatical theory of communication

if probability = 1 then information gained = 0 (p=1,I=0)

I(Xi) inversely proportional to P(Xi) probability..

I(Xi) = k(1/P(Xi))

______________________________________________________________

L1-part2

introduction to information==>

shanon==>mathematical theory of communication

p= probability

I(xi) inv. proportional to p(xi)

I(xi) = k(1/pxi) = log(1/p(xi))

==>properties of information

i) p=1; I(xi) = log(1) = 0

ii) p(xi) * p(yi) = log(1/(p(xi)p(yi)))  //independent ivents

		= -log{p(xi)p(yi)}

		= -log{p(xi)} - log{p(yi)}

		= I(xi) + I(yi)
 

UNITS OF INFORMATION

I(xi) = -log2{p(xi)} ---> bits

I(xi)  = -loge{p(xi)} ---->nats

I(xi) = -log10{p(xi)} ---> decit  
______________________________________________________________


Mutual Information

I{xi/yi} = -log{p(xi/yi)/p(xi)

}

______________________________________________________________

L2

entropy = randomness

entropy of message==>

Average self Information

	H(xi) = summision[i=1 to n]P(xi)I(xi)

Average Mutual Information:

	H(x:y)

Average Conditional self Information: 
	

Joint Entropy



______________________________________________________________

L3

shanon-fanno coding===>to convert info into data so that data can be send

===>used to reduce redundancy
===> to use B/w spectrum efficiency

Lavg =

Lmin = summision[i=1 to n]P(i)I(i)

efficiency(n) = Lmin/Lavg = H(xi)/Lavg

n% = Lmin/Lavg*100



______________________________________________________________

L4

==>used to reduce redundancy
==>to use bandwidth efficiently


______________________________________________________________

L5

channel==>noise and attenuation in channel==>

Discrete memoryless channel==>

xk(i/p) ===>| channel |===>Yk(o/p)

memoryless===>memory element are missing==>o/p only depends only on present i/p

channel matrix===>

1)Loseless channel

====>every column must have one non-zero value
===>summision of row= 1

2) Deterministic channel

3) Noiseless Channel===>diagonal elements are 1==>no noise interference

4) Binary Symmetric channel

______________________________________________________________

L6


shanon heartly channel cpacity theorem

channel capacity = Maximum Information Rate(C)

Information Rate (R)



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

22-08-2020

measurement of information ??
information is the uncertainty of the events...
more uncertainty, more information


probability 0 then more information===>

information inversaly proptional to probability
	
source=====>Modulator===>channel==>demodulator==>Destination


source coding==>is compression of redundant code

before modulator
source encoding ==>channel encoding

after demodulator
source decoder==>channel decoding

____________________________________________________________

24-08-2020

BOTH are mapping technique

source coding ===>is related to probability==>transmiting less and conveying maximum

Channel Coding :===>we are adding redundant bits in channel coding===>


Marginal probability ===>single event

Join probability ===>P(X,y

Bayes theorem / conditional Theorem p(Y|X) = P(X,Y)/P(X)

____________________________________________________________

25-08-2020

Amount of info. inv. proprtional to probability

random variable...?===> which can hold head or tail

==>A high probability event conveys less info. than a low probability information.

I(xi) ===>self information of an event xi

units of I(Xi) determines the base of logarithm

if log2 => bits

it i susually 2 means the unit is bits

since 0<= Pxi <= 1, so I(xi) >= 0

Entropy H(x)==>Average self information of a random variable X

____________________________________________________________

26-08-2020

Assignment ==> weekly==>

lab exercise ==>weekly

c progm ==>

i/p
____
1) no. of symbol
2) and their probability

o/p
____
1) self information of each symbol
2) entropy of sequence


____________________________________________________________

30-09-2020

==>Noiseless binary channel

 t=        0 1
	0| 1 0
	1| 0 1

===>Binary Symmetric channel

channel capacity= max of (Reduction = mutual information) by changing input probabiliteis

mutuak informaion = H(x) -H(x|y)






